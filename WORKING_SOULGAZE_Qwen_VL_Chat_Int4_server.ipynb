{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOrd/YTkT7225eurq/a32Ju"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUhE4Pff8sLO"
      },
      "outputs": [],
      "source": [
        "######### IMPORTANT ############\n",
        "#First, upload requirements.txt from here: https://huggingface.co/Qwen/Qwen-VL-Chat-Int4/blob/main/requirements.txt\n",
        "\n",
        "!pip install -r /content/requirements.txt\n",
        "!pip install flask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum gekko\n",
        "\n",
        "!git clone https://github.com/JustinLin610/AutoGPTQ.git\n",
        "%cd AutoGPTQ\n",
        "!pip install -v ."
      ],
      "metadata": {
        "id": "JqQhaTi9BHKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq tiktoken transformers_stream_generator accelerate einops"
      ],
      "metadata": {
        "id": "CQ2P73kaBLnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 2: Setting up Flask and Ngrok\n",
        "\n",
        "#Import the necessary modules and initialize Flask and Ngrok.\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import base64\n",
        "from PIL import Image\n",
        "import io\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Initialize the Flask application\n",
        "app = Flask(__name__)\n"
      ],
      "metadata": {
        "id": "J2qUrmgm88Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3: Load the Qwen LLM Model\n",
        "\n",
        "#Load the Qwen-VL-Chat LLM model.\n",
        "!pip install accelerate\n",
        "torch.manual_seed(1234)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat-Int4\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat-Int4\", device_map=\"cuda\", trust_remote_code=True).eval()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "htxbZyvv8_iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 4: Define the API Endpoint\n",
        "\n",
        "#Create an endpoint to handle POST requests, decode the image, and process it with the LLM.\n",
        "\n",
        "@app.route('/completion', methods=['POST'])\n",
        "def process_image():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "\n",
        "        # Extract the prompt and image data from the request\n",
        "        prompt = data['prompt']\n",
        "        image_data = data['image_data'][0]['data']  # Assuming one image per request for simplicity\n",
        "\n",
        "        # Decode the image\n",
        "        image_bytes = base64.b64decode(image_data)\n",
        "        image = Image.open(io.BytesIO(image_bytes))\n",
        "        image.save(\"temp_image.jpg\")  # Temporarily save the image to process\n",
        "\n",
        "        # Process with LLM\n",
        "        query = tokenizer.from_list_format([\n",
        "            {'image': 'temp_image.jpg'},\n",
        "            {'text': prompt},\n",
        "        ])\n",
        "        response, history = model.chat(tokenizer, query=query, history=None)\n",
        "\n",
        "        # Remove the temp image\n",
        "        os.remove(\"temp_image.jpg\")\n",
        "\n",
        "        return jsonify({\"content\": response})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VUM8jUWY9HI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f nohup.out\n",
        "!wget -c https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "!nohup ./cloudflared tunnel --url http://localhost:8007 &\n",
        "\n",
        "# Give it a moment to establish the tunnel\n",
        "!sleep 8\n",
        "\n",
        "# Retrieve the URL from the output\n",
        "!cat nohup.out | grep -o 'https://[a-zA-Z0-9-]*\\.trycloudflare.com'"
      ],
      "metadata": {
        "id": "hso7Asqep86b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 5: Start the Server\n",
        "\n",
        "#Add the command to run the Flask app.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=8007)\n"
      ],
      "metadata": {
        "id": "7hybkDUDxpaD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}